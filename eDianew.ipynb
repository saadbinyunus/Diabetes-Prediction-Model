{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install statsmodels\n",
    "%pip install statsmodels\n",
    "%pip install lightgbm\n",
    "%pip install scikit-learn\n",
    "%pip install seaborn\n",
    "%pip install matplotlib\n",
    "%pip install pandas\n",
    "%pip install numpy\n",
    "#Installation of required libraries\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import scale, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, mean_squared_error, r2_score, roc_auc_score, roc_curve, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action = \"ignore\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 9 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Pregnancies               768 non-null    int64  \n",
      " 1   Glucose                   768 non-null    int64  \n",
      " 2   BloodPressure             768 non-null    int64  \n",
      " 3   SkinThickness             768 non-null    int64  \n",
      " 4   Insulin                   768 non-null    int64  \n",
      " 5   BMI                       768 non-null    float64\n",
      " 6   DiabetesPedigreeFunction  768 non-null    float64\n",
      " 7   Age                       768 non-null    int64  \n",
      " 8   Outcome                   768 non-null    int64  \n",
      "dtypes: float64(2), int64(7)\n",
      "memory usage: 54.1 KB\n"
     ]
    }
   ],
   "source": [
    "#Reading the dataset\n",
    "df = pd.read_csv(\"data/diabetes.csv\")\n",
    "# The first 5 observation units of the data set were accessed.\n",
    "df.head()\n",
    "# The size of the data set was examined. It consists of 768 observation units and 9 variables.\n",
    "df.shape\n",
    "#Feature information\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features after RFE: Index(['Pregnancies', 'Glucose', 'BMI', 'DiabetesPedigreeFunction', 'Age'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Replace zeros in specific columns with median values\n",
    "columns_with_zeros = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
    "for column in columns_with_zeros:\n",
    "    df[column] = df[column].replace(0, df[column].median())\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df.drop([\"Outcome\"], axis=1)\n",
    "y = df[\"Outcome\"]\n",
    "\n",
    "# Apply MinMaxScaler for normalization\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Handle class imbalance using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Convert back to DataFrame with correct column names\n",
    "X_train_balanced = pd.DataFrame(X_train_balanced, columns=X_train.columns)\n",
    "\n",
    "# Feature Selection using Recursive Feature Elimination (RFE)\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "rfe = RFE(log_reg, n_features_to_select=5)\n",
    "rfe.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "selected_features = X_train.columns[rfe.support_]\n",
    "print(\"Selected Features after RFE:\", selected_features)\n",
    "\n",
    "# Select only the top features from X_train_balanced and X_test\n",
    "X_train_balanced = X_train_balanced[selected_features]\n",
    "X_test = X_test[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>30.5</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>30.5</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>23</td>\n",
       "      <td>30.5</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94.0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168.0</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35     30.5  33.6   \n",
       "1            1       85             66             29     30.5  26.6   \n",
       "2            8      183             64             23     30.5  23.3   \n",
       "3            1       89             66             23     94.0  28.1   \n",
       "4            0      137             40             35    168.0  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  \n",
       "0                     0.627   50  \n",
       "1                     0.351   31  \n",
       "2                     0.672   32  \n",
       "3                     0.167   21  \n",
       "4                     2.288   33  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression trained successfully!\n",
      "K-Nearest Neighbors trained successfully!\n",
      "Decision Tree trained successfully!\n",
      "Random Forest trained successfully!\n",
      "Support Vector Machine trained successfully!\n",
      "Gradient Boosting (XGB) trained successfully!\n"
     ]
    }
   ],
   "source": [
    "# Splitting data (assuming X and y are already defined)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12345)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=12345),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=12345),\n",
    "    'Random Forest': RandomForestClassifier(random_state=12345),\n",
    "    'Support Vector Machine': SVC(gamma='auto', random_state=12345),\n",
    "    'Gradient Boosting (XGB)': GradientBoostingClassifier(random_state=12345)\n",
    "}\n",
    "\n",
    "# Train all models\n",
    "trained_models = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_balanced, y_train_balanced)  # Train the model with balanced data\n",
    "    trained_models[name] = model  # Store trained model\n",
    "    print(f\"{name} trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Single Entry Predictions:\n",
      "Logistic Regression: Predicted=1, Actual=0\n",
      "K-Nearest Neighbors: Predicted=1, Actual=0\n",
      "Decision Tree: Predicted=1, Actual=0\n",
      "Random Forest: Predicted=1, Actual=0\n",
      "Support Vector Machine: Predicted=1, Actual=0\n",
      "Gradient Boosting (XGB): Predicted=1, Actual=0\n"
     ]
    }
   ],
   "source": [
    "# Select only the features from the RFE result\n",
    "selected_features = ['Pregnancies', 'Glucose', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n",
    "\n",
    "# Select a single test entry\n",
    "single_entry = X_test.iloc[0:1][selected_features]  # Keep DataFrame format and select relevant features\n",
    "actual_label = y_test.iloc[0]  # Get actual label\n",
    "\n",
    "# Evaluate single entry with each model\n",
    "print(\"\\nSingle Entry Predictions:\")\n",
    "for name, model in trained_models.items():\n",
    "    predicted_label = model.predict(single_entry)\n",
    "    print(f\"{name}: Predicted={predicted_label[0]}, Actual={actual_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Predictions on Multiple Test Entries:\n",
      "Sample Index Actual Logistic Regression K-Nearest Neighbors Decision Tree Random Forest Support Vector Machine Gradient Boosting (XGB)\n",
      "          44      0                   1                   1             1             1                      1                       1\n",
      "         672      0                   0                   0             0             0                      0                       0\n",
      "         700      0                   0                   0             0             0                      0                       0\n",
      "         630      1                   0                   1             0             0                      0                       0\n",
      "          81      0                   0                   0             0             0                      0                       0\n",
      "         389      0                   0                   0             0             1                      0                       0\n",
      "         387      1                   1                   0             0             0                      1                       1\n",
      "         408      1                   1                   1             0             1                      1                       1\n",
      "         163      0                   0                   0             0             0                      0                       0\n",
      "         335      0                   1                   1             1             1                      1                       1\n"
     ]
    }
   ],
   "source": [
    "# Select multiple test entries\n",
    "num_samples = 10  # Change this to however many samples you want to test\n",
    "X_samples = X_test.iloc[:num_samples]  # Select the first `num_samples` entries\n",
    "y_samples = y_test.iloc[:num_samples]  # Get corresponding actual labels\n",
    "\n",
    "# Create a DataFrame to store results\n",
    "results_df = pd.DataFrame(columns=['Sample Index', 'Actual'] + list(trained_models.keys()))\n",
    "\n",
    "# Evaluate multiple entries with each model\n",
    "for i, (idx, actual_label) in enumerate(y_samples.items()):\n",
    "    row = {'Sample Index': idx, 'Actual': actual_label}  # Store sample index and actual label\n",
    "    for name, model in trained_models.items():\n",
    "        predicted_label = model.predict([X_samples.iloc[i]])[0]  # Predict for one row\n",
    "        row[name] = predicted_label  # Store prediction\n",
    "    \n",
    "    # Use pd.concat instead of append to add the row to the DataFrame\n",
    "    results_df = pd.concat([results_df, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nModel Predictions on Multiple Test Entries:\")\n",
    "print(results_df.to_string(index=False))  # Print without DataFrame index\n",
    "\n",
    "results_df.to_csv(\"model_predictions_test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Majority Vote Predictions:\n",
      "Sample Index Actual Final Prediction\n",
      "         245      1                1\n",
      "         473      0                1\n",
      "         206      1                1\n",
      "         204      0                0\n",
      "         655      1                0\n",
      "         275      0                0\n",
      "         583      0                0\n",
      "         264      1                0\n",
      "         401      0                0\n",
      "         122      0                0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'precision_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 59\u001b[0m\n\u001b[0;32m     56\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m voting_clf\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     58\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, y_pred)\n\u001b[1;32m---> 59\u001b[0m precision \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_score\u001b[49m(y_test, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# 'weighted' averages scores based on class distribution\u001b[39;00m\n\u001b[0;32m     60\u001b[0m recall \u001b[38;5;241m=\u001b[39m recall_score(y_test, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     61\u001b[0m f1 \u001b[38;5;241m=\u001b[39m f1_score(y_test, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'precision_score' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Split dataset (assumes X and y are defined)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=12345)\n",
    "\n",
    "# Scale features for better performance\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define individual models\n",
    "models = [\n",
    "    ('LR', LogisticRegression(random_state=12345)),\n",
    "    ('KNN', KNeighborsClassifier()),\n",
    "    ('CART', DecisionTreeClassifier(random_state=12345)),\n",
    "    ('RF', RandomForestClassifier(n_estimators=200, random_state=12345)),\n",
    "    ('SVM', SVC(gamma='auto', random_state=12345)),\n",
    "    ('XGB', GradientBoostingClassifier(n_estimators=200, random_state=12345))\n",
    "]\n",
    "\n",
    "# Create a Voting Classifier with majority voting\n",
    "voting_clf = VotingClassifier(estimators=models, voting='hard')  # 'hard' for majority vote\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions for multiple test samples\n",
    "num_samples = 10  # Change this to however many test samples you want\n",
    "X_samples = X_test[:num_samples]\n",
    "y_samples = y_test[:num_samples]\n",
    "\n",
    "# Store results in a DataFrame\n",
    "results_df = pd.DataFrame(columns=['Sample Index', 'Actual', 'Final Prediction'])\n",
    "\n",
    "# Evaluate and print results\n",
    "for i, (idx, actual_label) in enumerate(y_samples.items()):\n",
    "    final_prediction = voting_clf.predict([X_samples[i]])[0]  # Get majority vote prediction\n",
    "    row = {'Sample Index': idx, 'Actual': actual_label, 'Final Prediction': final_prediction}\n",
    "    \n",
    "    # Use pd.concat to add the row to the DataFrame\n",
    "    results_df = pd.concat([results_df, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nMajority Vote Predictions:\")\n",
    "print(results_df.to_string(index=False))  # Print nicely formatted table\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Evaluate performance metrics\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')  # 'weighted' averages scores based on class distribution\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print performance metrics\n",
    "print(\"\\nPerformance Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision (weighted): {precision:.4f}\")\n",
    "print(f\"Recall (weighted): {recall:.4f}\")\n",
    "print(f\"F1 Score (weighted): {f1:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
