{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: seaborn in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.13.0)\n",
      "Requirement already satisfied: xgboost in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.14.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (4.55.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn) (0.1.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "#%pip install numpy pandas scikit-learn statsmodels lightgbm seaborn matplotlib imbalanced-learn\n",
    "#%pip install xgboost\n",
    "#%pip install scipy\n",
    "%pip install numpy pandas seaborn matplotlib scikit-learn imbalanced-learn xgboost scipy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_selection import RFE\n",
    "import warnings\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.simplefilter(action=\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 9 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Pregnancies               768 non-null    int64  \n",
      " 1   Glucose                   768 non-null    int64  \n",
      " 2   BloodPressure             768 non-null    int64  \n",
      " 3   SkinThickness             768 non-null    int64  \n",
      " 4   Insulin                   768 non-null    int64  \n",
      " 5   BMI                       768 non-null    float64\n",
      " 6   DiabetesPedigreeFunction  768 non-null    float64\n",
      " 7   Age                       768 non-null    int64  \n",
      " 8   Outcome                   768 non-null    int64  \n",
      "dtypes: float64(2), int64(7)\n",
      "memory usage: 54.1 KB\n",
      "None\n",
      "\n",
      "First 5 rows of the dataset:\n",
      "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
      "0            6      148             72             35        0  33.6   \n",
      "1            1       85             66             29        0  26.6   \n",
      "2            8      183             64              0        0  23.3   \n",
      "3            1       89             66             23       94  28.1   \n",
      "4            0      137             40             35      168  43.1   \n",
      "\n",
      "   DiabetesPedigreeFunction  Age  Outcome  \n",
      "0                     0.627   50        1  \n",
      "1                     0.351   31        0  \n",
      "2                     0.672   32        1  \n",
      "3                     0.167   21        0  \n",
      "4                     2.288   33        1  \n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"data/diabetes.csv\")\n",
    "\n",
    "# Display dataset information\n",
    "print(\"Dataset Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace zeros with median for specific columns\n",
    "columns_with_zeros = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
    "for column in columns_with_zeros:\n",
    "    df[column] = df[column].replace(0, df[column].median())\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df.drop([\"Outcome\"], axis=1)\n",
    "y = df[\"Outcome\"]\n",
    "\n",
    "# Apply MinMaxScaler for normalization\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected Features after RFE: Index(['Pregnancies', 'Glucose', 'BMI', 'DiabetesPedigreeFunction', 'Age'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Feature Selection using Recursive Feature Elimination (RFE)\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "rfe = RFE(log_reg, n_features_to_select=5)\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "selected_features = X_train.columns[rfe.support_]\n",
    "print(\"\\nSelected Features after RFE:\", selected_features)\n",
    "\n",
    "# Select only the top features from X_train and X_test\n",
    "X_train = X_train[selected_features]\n",
    "X_test = X_test[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE to handle class imbalance\n",
    "smote = SMOTE(random_state=12345)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for XGBoost: {'colsample_bytree': np.float64(0.9113943388538406), 'gamma': np.float64(0.1992471753294794), 'learning_rate': np.float64(0.19734464063457888), 'max_depth': 8, 'n_estimators': 361, 'reg_alpha': np.float64(0.4945498003146045), 'reg_lambda': np.float64(0.03738037598980304), 'subsample': np.float64(0.8876825332624142)}\n",
      "Best Parameters for Random Forest: {'bootstrap': True, 'max_depth': 10, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 11, 'n_estimators': 367}\n",
      "Best Parameters for SVM: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "Best Parameters for Decision Tree: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 10}\n",
      "Best Parameters for Neural Network: {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'max_iter': 300}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Hyperparameter tuning for XGBoost\n",
    "# Expanded hyperparameter grid\n",
    "param_dist_xgb = {\n",
    "    'n_estimators': randint(100, 500),  # Randomly sample between 100 and 500\n",
    "    'max_depth': randint(3, 10),  # Randomly sample between 3 and 10\n",
    "    'learning_rate': uniform(0.01, 0.3),  # Randomly sample between 0.01 and 0.3\n",
    "    'subsample': uniform(0.6, 0.4),  # Randomly sample between 0.6 and 1.0\n",
    "    'colsample_bytree': uniform(0.6, 0.4),  # Randomly sample between 0.6 and 1.0\n",
    "    'gamma': uniform(0, 0.5),  # Randomly sample between 0 and 0.5\n",
    "    'reg_alpha': uniform(0, 1),  # L1 regularization (randomly sample between 0 and 1)\n",
    "    'reg_lambda': uniform(0, 1)  # L2 regularization (randomly sample between 0 and 1)\n",
    "}\n",
    "\n",
    "# Initialize XGBClassifier with early_stopping_rounds\n",
    "xgb = XGBClassifier(random_state=12345, scale_pos_weight=1, early_stopping_rounds=10)\n",
    "\n",
    "# Use RandomizedSearchCV for efficient hyperparameter tuning\n",
    "random_search_xgb = RandomizedSearchCV(\n",
    "    estimator=xgb, \n",
    "    param_distributions=param_dist_xgb, \n",
    "    n_iter=50,  # Number of parameter settings to sample\n",
    "    cv=10,  # 10-fold cross-validation\n",
    "    scoring='accuracy', \n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    random_state=12345\n",
    ")\n",
    "\n",
    "# Fit the model with early stopping\n",
    "random_search_xgb.fit(\n",
    "    X_train_balanced, \n",
    "    y_train_balanced, \n",
    "    eval_set=[(X_train_balanced, y_train_balanced)],  # Evaluation set for early stopping\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"Best Parameters for XGBoost:\", random_search_xgb.best_params_)\n",
    "# Hyperparameter tuning for Random Forest\n",
    "param_dist_rf = {\n",
    "    'n_estimators': randint(100, 500),  # Randomly sample between 100 and 500\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],  # Maximum depth of the tree\n",
    "    'min_samples_split': randint(2, 20),  # Randomly sample between 2 and 20\n",
    "    'min_samples_leaf': randint(1, 10),  # Randomly sample between 1 and 10\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],  # Different strategies for feature selection\n",
    "    'bootstrap': [True, False]  # Whether to use bootstrap samples\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=12345, class_weight='balanced')\n",
    "\n",
    "# Use RandomizedSearchCV for more efficient hyperparameter tuning\n",
    "random_search_rf = RandomizedSearchCV(\n",
    "    estimator=rf, \n",
    "    param_distributions=param_dist_rf, \n",
    "    n_iter=50,  # Number of parameter settings to sample\n",
    "    cv=10,  # 10-fold cross-validation\n",
    "    scoring='accuracy', \n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    random_state=12345\n",
    ")\n",
    "\n",
    "random_search_rf.fit(X_train_balanced, y_train_balanced)\n",
    "print(\"Best Parameters for Random Forest:\", random_search_rf.best_params_)\n",
    "\n",
    "# Hyperparameter tuning for SVM\n",
    "param_grid_svm = {\n",
    "    'C': [0.1, 1, 10],  # Regularization parameter\n",
    "    'gamma': ['scale', 'auto', 0.1],  # Kernel coefficient\n",
    "    'kernel': ['linear', 'rbf']  # Kernel type\n",
    "}\n",
    "\n",
    "svm = SVC(random_state=12345, probability=True)\n",
    "grid_search_svm = GridSearchCV(\n",
    "    estimator=svm, \n",
    "    param_grid=param_grid_svm, \n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='accuracy', \n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "grid_search_svm.fit(X_train_balanced, y_train_balanced)\n",
    "print(\"Best Parameters for SVM:\", grid_search_svm.best_params_)\n",
    "\n",
    "# Hyperparameter tuning for Decision Tree\n",
    "param_grid_dt = {\n",
    "    'max_depth': [None, 10, 20, 30],  # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split a node\n",
    "    'min_samples_leaf': [1, 2, 4],  # Minimum number of samples required at a leaf node\n",
    "    'criterion': ['gini', 'entropy']  # Splitting criterion\n",
    "}\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=12345)\n",
    "grid_search_dt = GridSearchCV(\n",
    "    estimator=dt, \n",
    "    param_grid=param_grid_dt, \n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='accuracy', \n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "grid_search_dt.fit(X_train_balanced, y_train_balanced)\n",
    "print(\"Best Parameters for Decision Tree:\", grid_search_dt.best_params_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameter tuning for Neural Network (MLPClassifier)\n",
    "param_grid_nn = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50)],  # Number of neurons in each hidden layer\n",
    "    'activation': ['relu', 'tanh'],  # Activation function\n",
    "    'alpha': [0.0001, 0.001, 0.01],  # L2 regularization term\n",
    "    'learning_rate': ['constant', 'adaptive'],  # Learning rate schedule\n",
    "    'max_iter': [200, 300]  # Maximum number of iterations\n",
    "}\n",
    "\n",
    "# Initialize MLPClassifier\n",
    "nn = MLPClassifier(random_state=12345)\n",
    "\n",
    "# Perform GridSearchCV for MLPClassifier\n",
    "grid_search_nn = GridSearchCV(\n",
    "    estimator=nn, \n",
    "    param_grid=param_grid_nn, \n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='accuracy', \n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "grid_search_nn.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Print best parameters\n",
    "print(\"Best Parameters for Neural Network:\", grid_search_nn.best_params_)\n",
    "\n",
    "# Define and train individual models with best hyperparameters\n",
    "models = {\n",
    "    'XGBoost': XGBClassifier(random_state=12345, **random_search_xgb.best_params_),\n",
    "    'Random Forest': RandomForestClassifier(random_state=12345, **random_search_rf.best_params_),\n",
    "    'Support Vector Machine': SVC(random_state=12345, probability=True, **grid_search_svm.best_params_),\n",
    "    'Neural Network': MLPClassifier(random_state=12345, **grid_search_nn.best_params_),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=12345, **grid_search_dt.best_params_)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XGBoost Performance Metrics:\n",
      "Accuracy: 0.7078\n",
      "Sensitivity: 0.6296\n",
      "Specificity: 0.7500\n",
      "Gini Index: 0.5856\n",
      "AUC: 0.7928\n",
      "AUCH: 0.7928\n",
      "MER: 0.2922\n",
      "MWC: 0.6898\n",
      "Precision: 0.7147\n",
      "\n",
      "Random Forest Performance Metrics:\n",
      "Accuracy: 0.7208\n",
      "Sensitivity: 0.6852\n",
      "Specificity: 0.7400\n",
      "Gini Index: 0.6378\n",
      "AUC: 0.8189\n",
      "AUCH: 0.8189\n",
      "MER: 0.2792\n",
      "MWC: 0.7126\n",
      "Precision: 0.7340\n",
      "\n",
      "Support Vector Machine Performance Metrics:\n",
      "Accuracy: 0.7208\n",
      "Sensitivity: 0.7037\n",
      "Specificity: 0.7300\n",
      "Gini Index: 0.6000\n",
      "AUC: 0.8000\n",
      "AUCH: 0.8000\n",
      "MER: 0.2792\n",
      "MWC: 0.7169\n",
      "Precision: 0.7376\n",
      "\n",
      "Neural Network Performance Metrics:\n",
      "Accuracy: 0.7532\n",
      "Sensitivity: 0.8148\n",
      "Specificity: 0.7200\n",
      "Gini Index: 0.6252\n",
      "AUC: 0.8126\n",
      "AUCH: 0.8126\n",
      "MER: 0.2468\n",
      "MWC: 0.7674\n",
      "Precision: 0.7844\n",
      "\n",
      "Decision Tree Performance Metrics:\n",
      "Accuracy: 0.7597\n",
      "Sensitivity: 0.6852\n",
      "Specificity: 0.8000\n",
      "Gini Index: 0.4852\n",
      "AUC: 0.7426\n",
      "AUCH: 0.7426\n",
      "MER: 0.2403\n",
      "MWC: 0.7426\n",
      "Precision: 0.7632\n"
     ]
    }
   ],
   "source": [
    "# Define and train individual models\n",
    "models = {\n",
    "    'XGBoost': XGBClassifier(random_state=12345, **random_search_xgb.best_params_),\n",
    "    'Random Forest': RandomForestClassifier(random_state=12345, **random_search_rf.best_params_),\n",
    "    'Support Vector Machine': SVC(random_state=12345, probability=True, **grid_search_svm.best_params_),\n",
    "    'Neural Network': MLPClassifier(random_state=12345, **grid_search_nn.best_params_),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=12345, **grid_search_dt.best_params_)\n",
    "}\n",
    "# Function to calculate additional metrics\n",
    "def calculate_metrics(y_true, y_pred, y_pred_proba=None):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    gini_index = 2 * roc_auc_score(y_true, y_pred_proba) - 1 if y_pred_proba is not None else None\n",
    "    auc_score = roc_auc_score(y_true, y_pred_proba) if y_pred_proba is not None else None\n",
    "    auch = auc_score  # Approximate AUCH as AUC\n",
    "    mer = 1 - accuracy_score(y_true, y_pred)\n",
    "    mwc = (sensitivity + specificity) / 2\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),\n",
    "        'Sensitivity': sensitivity,\n",
    "        'Specificity': specificity,\n",
    "        'Gini Index': gini_index,\n",
    "        'AUC': auc_score,\n",
    "        'AUCH': auch,\n",
    "        'MER': mer,\n",
    "        'MWC': mwc,\n",
    "        'Precision': precision\n",
    "    }\n",
    "\n",
    "# Train and evaluate each model\n",
    "performance_metrics = {}\n",
    "for name, model in models.items():\n",
    "    # Train the model\n",
    "    model.fit(X_train_balanced, y_train_balanced)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    metrics = calculate_metrics(y_test, y_pred, y_pred_proba)\n",
    "    \n",
    "    # Store metrics\n",
    "    performance_metrics[name] = metrics\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"\\n{name} Performance Metrics:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ensemble Model (Voting Classifier) Performance Metrics:\n",
      "Accuracy: 0.7727\n",
      "Sensitivity: 0.7593\n",
      "Specificity: 0.7800\n",
      "Gini Index: 0.6496\n",
      "AUC: 0.8248\n",
      "AUCH: 0.8248\n",
      "MER: 0.2273\n",
      "MWC: 0.7696\n",
      "Precision: 0.7848\n"
     ]
    }
   ],
   "source": [
    "# Define individual models for ensemble\n",
    "models = [\n",
    "    ('XGBoost', XGBClassifier(random_state=12345, **grid_search_xgb.best_params_)),\n",
    "    ('Random Forest', RandomForestClassifier(random_state=12345)),\n",
    "    ('Support Vector Machine', SVC(gamma='auto', random_state=12345, probability=True)),\n",
    "    ('Neural Network', MLPClassifier(random_state=12345)),\n",
    "    ('Decision Tree', DecisionTreeClassifier(random_state=12345))\n",
    "]\n",
    "\n",
    "# Create a Voting Classifier with majority voting\n",
    "voting_clf = VotingClassifier(estimators=models, voting='soft')  # 'soft' for probability-based voting\n",
    "voting_clf.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Evaluate performance metrics\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "y_pred_proba = voting_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = calculate_metrics(y_test, y_pred, y_pred_proba)\n",
    "print(\"\\nEnsemble Model (Voting Classifier) Performance Metrics:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance Summary:\n",
      "Model                Accuracy   Sensitivity Specificity GI         AUC        AUCH       MER        MWC        Precision \n",
      "XGBoost              0.7078     0.6296     0.7500     0.5856     0.7928     0.7928     0.2922     0.6898     0.7147    \n",
      "Random Forest        0.7208     0.6852     0.7400     0.6378     0.8189     0.8189     0.2792     0.7126     0.7340    \n",
      "Support Vector Machine 0.7208     0.7037     0.7300     0.6000     0.8000     0.8000     0.2792     0.7169     0.7376    \n",
      "Neural Network       0.7532     0.8148     0.7200     0.6252     0.8126     0.8126     0.2468     0.7674     0.7844    \n",
      "Decision Tree        0.7597     0.6852     0.8000     0.4852     0.7426     0.7426     0.2403     0.7426     0.7632    \n"
     ]
    }
   ],
   "source": [
    "# Print results in a table format\n",
    "print(\"\\nModel Performance Summary:\")\n",
    "print(\"{:<20} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10}\".format(\n",
    "    \"Model\", \"Accuracy\", \"Sensitivity\", \"Specificity\", \"GI\", \"AUC\", \"AUCH\", \"MER\", \"MWC\", \"Precision\"\n",
    "))\n",
    "for name, metrics in performance_metrics.items():\n",
    "    print(\"{:<20} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f}\".format(\n",
    "        name, metrics['Accuracy'], metrics['Sensitivity'], metrics['Specificity'], \n",
    "        metrics['Gini Index'], metrics['AUC'], metrics['AUCH'], metrics['MER'], metrics['MWC'], metrics['Precision']\n",
    "    ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
